## Convolutional Neural Network(CNN)
----
>**Neural Network and Deep Learning(邱锡鹏，2020)**[[Book]](https://nndl.github.io/)<br/>
> #### Appendix.A 线性代数(Linear Algebra)
> + N维向量
>    + 标量(Scalar)：是一个实数，只有大小，没有方向。e.g.,*a,b,c,...*
>    + 向量(Vector)：是一组实数构成的有序数组，有大小，有方向.e.g.,N维向量<!-- $a=[a_1, a_2, ..., a_N].$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%3D%5Ba_1%2C%20a_2%2C%20...%2C%20a_N%5D"> 
>    + 向量空间(Vector Space or Linear Space)：向量组成的集合，且满足*向量加法*和*标量乘法*（两者可视为向量空间的条件）.
>       + 欧氏空间(Euclidean Space)：一个欧氏空间通常表示为<!-- $\mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathbb%7BR%7D%5EN">，N为空间维度(Dimension).
>          + 欧氏空间中的*向量加法*: <br/><!-- $a+b=[a_1, a_2, ..., a_N]+[b_1, b_2, ..., b_N]=[a_1+b_1,a_2+b_2,...,a_N+b_N].$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%2Bb%3D%5Ba_1%2C%20a_2%2C%20...%2C%20a_N%5D%2B%5Bb_1%2C%20b_2%2C%20...%2C%20b_N%5D%3D%5Ba_1%2Bb_1%2Ca_2%2Bb_2%2C...%2Ca_N%2Bb_N%5D"> 
>          + 欧氏空间中的*标量乘法*：<br/><!-- $c \cdot a = c \cdot [a_1, a_2, ..., a_N]=[ca_1, ca_2, ..., ca_N].$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=c%20%5Ccdot%20a%20%3D%20c%20%5Ccdot%20%5Ba_1%2C%20a_2%2C%20...%2C%20a_N%5D%3D%5Bca_1%2C%20ca_2%2C%20...%2C%20ca_N%5D."> 其中<!-- $a,b,c \in \mathbb{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%2Cb%2Cc%20%5Cin%20%5Cmathbb%7BR%7D"> 属于标量.
>       + 线性子空间：向量空间的一个子集，也满足*向量加法*和*标量乘法*.
>       + 线性相关(linearly dependent)：如果向量空间中的一个向量可以用有限个其他向量的线性组合表示，则称线性相关.<br/>或线性空间<!-- $\mathbb{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathbb%7Bv%7D">中的**M**个向量<!-- $\lbrace v_1, v_2,...,v_M \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7Bv_1%2C%20v_2%2C...%2Cv_M%7D">，如果<!-- $\lambda_1 v_1+\lambda_2 v_2+...+\lambda_M v_M=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda_1%20v_1%2B%5Clambda_2%20v_2%2B...%2B%5Clambda_M%20v_M%3D0">存在非零解<!-- $\lambda_1,\lambda_2,...,\lambda_M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda_1%2C%5Clambda_2%2C...%2C%5Clambda_M">（标量），则向量组<!-- $\lbrace v_1, v_2,...,v_M \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clbrace%20v_1%2C%20v_2%2C...%2Cv_M%20%5Crbrace">是线性相关的；否则向量组<!-- $\lbrace v_1, v_2,...,v_M \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clbrace%20v_1%2C%20v_2%2C...%2Cv_M%20%5Crbrace">是线性无关的(linearly independent).
>        + 基向量：N维向量空间<!-- $\mathcal{V}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BV%7D">的基(Base)<!-- $\mathcal{B}=\lbrace e_1,e_2,...,e_N \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BB%7D%3D%5Clbrace%20e_1%2Ce_2%2C...%2Ce_N%20%5Crbrace">是<!-- $\mathbb{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathbb%7Bv%7D">的有限子集，其元素之间线性无关。向量空间<!-- $\mathbb{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathbb%7Bv%7D">的所有向量都可以按唯一的方式表达为<!-- $\mathcal{B}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BB%7D">中向量的线性组合，即对于任意<!-- $v \in \mathcal{V}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v%20%5Cin%20%5Cmathcal%7BV%7D">存在一组标量<!-- $\lbrace \lambda_1,\lambda_2,...,\lambda_N \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clbrace%20%5Clambda_1%2C%5Clambda_2%2C...%2C%5Clambda_N%20%5Crbrace">，使得<!-- $v=\lambda_1e_1+\lambda_2e_2+...+\lambda_Ne_N.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v%3D%5Clambda_1e_1%2B%5Clambda_2e_2%2B...%2B%5Clambda_Ne_N"> 且如果基向量是有序的，则<!-- $(\lambda_1,\lambda_2,...,\lambda_N)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(%5Clambda_1%2C%5Clambda_2%2C...%2C%5Clambda_N)">称为向量<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">关于基<!-- $\mathcal{B}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BB%7D">的坐标.如果基向量是Standard Basis,<!-- $(\lambda_1,\lambda_2,...,\lambda_N)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(%5Clambda_1%2C%5Clambda_2%2C...%2C%5Clambda_N)">则可称为向量<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">的笛卡尔坐标(Cartesian Coordinate).
>        + 内积(Inner Product|Dot Product|Scalar Product)：一个N维向量空间中的2个向量***a***和***b***的内积为<!-- $<a,b>=\sum_{n-1}^Na_nb_n$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%3Ca%2Cb%3E%3D%5Csum_%7Bn-1%7D%5ENa_nb_n">(基向量相同,<!-- $cos\theta=1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=cos%5Ctheta%3D1">).
>        + 正交(Orthogonal)：如果同一个向量空间中2个向量的内积为0，则这两个向量正交；如果向量空间A中的一个向量<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">与子空间<!-- $\mathcal{U}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BU%7D">的每个向量正交，那么向量<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">与子空间<!-- $\mathcal{U}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BU%7D">正交.
>    + 范数(Norm)：一个表示向量“长度”的函数，为向量空间内的所有向量赋予*非零*的正长度或大小.对于一个N维向量<!-- $\mathcal{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bv%7D">, 常见的一个范数函数<!-- $\mathcal{l}_p$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_p">范数，<!-- $\mathcal{l}_p(\mathcal{v}) = \begin{Vmatrix} \mathcal{v} \end{Vmatrix}_p=(\sum_{n=1}^N\begin{vmatrix}v_n\end{vmatrix}^p)^{\frac 1p}$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\icrSLi5ixn.svg">. 其中<!-- $p\ge 0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p%5Cge%200">为这个标量的参数，常见取值有<!-- $1,2,\infty$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=1%2C2%2C%5Cinfty">.
>        + <!-- $\mathcal{l}_1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_1">范数：<!-- $\begin{Vmatrix} \mathcal{v} \end{Vmatrix}_1=\sum_{n=1}^N \begin{vmatrix} \mathcal{v}_n \end{vmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7BVmatrix%7D%20%5Cmathcal%7Bv%7D%20%5Cend%7BVmatrix%7D_1%3D%5Csum_%7Bn%3D1%7D%5EN%20%5Cbegin%7Bvmatrix%7D%20%5Cmathcal%7Bv%7D_n%20%5Cend%7Bvmatrix%7D."> 向量的各个元素的绝对值之和.
>        + <!-- $\mathcal{l}_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_2">范数：<br/>
>           <!-- $\begin{Vmatrix} \mathcal{v} \end{Vmatrix}_2=\sqrt{\sum_{n=1}^N \mathcal{v}_n^2}=\sqrt{\mathcal{v}^T\mathcal{v}}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7BVmatrix%7D%20%5Cmathcal%7Bv%7D%20%5Cend%7BVmatrix%7D_2%3D%5Csqrt%7B%5Csum_%7Bn%3D1%7D%5EN%20%5Cmathcal%7Bv%7D_n%5E2%7D%3D%5Csqrt%7B%5Cmathcal%7Bv%7D%5ET%5Cmathcal%7Bv%7D%7D."> 向量的各个元素的平方和再开平方. <!-- $\mathcal{l}_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_2">范数又称Euclidean范数或者Frobenius范数或者向量的*模*.从几何角度，向量是从原点出发的一个带箭头的有向线段，<!-- $\mathcal{l}_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_2">范数是线段的长度。
>        + <!-- $\mathcal{l}_\infty$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_%5Cinfty">范数：
>          <!-- $\begin{Vmatrix}\mathcal{v}\end{Vmatrix}_\infty=max \lbrace v_1,v_2,...,v_N \rbrace.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7BVmatrix%7D%5Cmathcal%7Bv%7D%5Cend%7BVmatrix%7D_%5Cinfty%3Dmax%20%5Clbrace%20v_1%2Cv_2%2C...%2Cv_N%20%5Crbrace."> 向量的各个元素的*最大绝对值*.
>    + 常见向量：
>       + 全0向量：所有元素都为0的向量，表示为0，是笛卡尔坐标系中的原点.
>       + 全1向量：所有元素都为1的向量，表示为1.
>       + one-hot向量：有且只有一个元素为1，其他元素都为0的向量.
> + 矩阵
>    + 线性映射(Linear Mapping)：即线性变换，可理解为一个映射函数，把*向量*从线性空间<!-- $\mathcal{X}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BX%7D">映射到线性空间<!-- $\mathcal{Y}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BY%7D">，表示为<!-- $f:\mathcal{X} \to \mathcal{Y}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f%3A%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathcal%7BY%7D">，并满足：<br/>
> 对于<!-- $\mathcal{X}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BX%7D">中任意两个向量<!-- $u$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=u">和<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">，以及任意标量<!-- $c$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=c">，有<!-- $f(u+v)=f(u)+f(v), f(c \cdot v)=cf(v)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(u%2Bv)%3Df(u)%2Bf(v)%2C%20f(c%20%5Ccdot%20v)%3Dcf(v)">.
>       + 例：函数<!-- $f:\mathbb{R}^N \to \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f%3A%5Cmathbb%7BR%7D%5EN%20%5Cto%20%5Cmathbb%7BR%7D%5EM"><br/><!-- $y=Ax={\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1N}\\a_{21} & a_{22} & \cdots & a_{2N}\\\vdots & \vdots & \ddots & \vdots\\a_{M1} & a_{M2} &  \cdots & a_{MN}\end{bmatrix}}\times{\begin{bmatrix}x_{1}\\x_{2}\\\vdots\\x_N\end{bmatrix}}={\begin{bmatrix}a_{11}x_1+a_{12}x_2+\cdots+a_{1N}x_N\\a_{21}x_1+a_{22}x_2+\cdots+a_{2N}x_N\\\vdots\\a_{M1}x_1+a_{M2}x_2+\cdots+a_{MN}x_N\end{bmatrix}}={\begin{bmatrix}y_1\\y_2\\\vdots\\y_M\end{bmatrix}}.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\PmYfqO6MtI.svg"><br/><!-- $(A \in \mathbb{R}^{M \times N}, x \in \mathbb{R}^N, y \in \mathbb{R}^M).$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%20x%20%5Cin%20%5Cmathbb%7BR%7D%5EN%2C%20y%20%5Cin%20%5Cmathbb%7BR%7D%5EM).">  
>    + 仿射变换(Affine Transformation)：通过一个线性变换和一个平移，实现一个线性空间的旋转、平移、缩放变换. 表示为：<br/><!-- $y=Ax+b, A \in \mathbb{R}^{N \times N},x \in \mathbb{R}^N,b \in \mathbb{R}^N.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\M8QW8V3uK6.svg">
>       + 当<!-- $b=0$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\lWxuwPVEPe.svg">时，*仿射变换*就退化为*线性变换*.
>       + 仿射变换不会改变原始空间中的相对位置关系，具有4个性质：<br/>1. 共线性：同一条直线上的三个及以上的点变换后依然在一条直线上.<br/>2. 比例不变：不同点之间的距离比例不变.<br/>3. 平行性不变：两条平行线在转化后依然会平行.<br/>4. 凸性不变：凸集(Convex set)转换后依然是凸的.
>          + 凸性: <!-- $\forall {x_1,x_2} \in C,\forall \theta \in [0,1],\theta x_1+(1-\theta)x_2 \in C.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cforall%20%7Bx_1%2Cx_2%7D%20%5Cin%20C%2C%5Cforall%20%5Ctheta%20%5Cin%20%5B0%2C1%5D%2C%5Ctheta%20x_1%2B(1-%5Ctheta)x_2%20%5Cin%20C.">如果集合C中任意两点之间的线段都在C中，则该集合是凸的.<br/>
\* 凸优化的延伸介绍(今天数学学点啥，2020)[[知乎]](https://zhuanlan.zhihu.com/p/94879910?from_voters_page=true)
>    + 矩阵操作：
>       + 加：<!-- $A,B \in \mathbb{R}^{M \times N},[A+B]_{mn}=a_{mn}+b_{mn}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%2CB%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%5BA%2BB%5D_%7Bmn%7D%3Da_%7Bmn%7D%2Bb_%7Bmn%7D.">
>       + 乘积：两个矩阵的乘积仅当第一个矩阵的*列数*和第二个矩阵的*行数*相等时才能定义，如：<!-- $A \in \mathbb{R}^{M \times K},B \in \mathbb{R}^{K \times N}, then AB \in \mathbb{R}^{M \times N},[AB]_{mn}=\sum_{k=1}^K a_{mk}b_{kn}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20K%7D%2CB%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BK%20%5Ctimes%20N%7D%2C%20then%20AB%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%5BAB%5D_%7Bmn%7D%3D%5Csum_%7Bk%3D1%7D%5EK%20a_%7Bmk%7Db_%7Bkn%7D.">
>          + 矩阵乘法满足*结合律*和*分配律*. 如：<br/>用矩阵A和B分别表示线性映射<!-- $f:\mathbb{R}^N \to \mathbb{R}^K, g:\mathbb{R}^K \to \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f%3A%5Cmathbb%7BR%7D%5EN%20%5Cto%20%5Cmathbb%7BR%7D%5EK%2C%20g%3A%5Cmathbb%7BR%7D%5EK%20%5Cto%20%5Cmathbb%7BR%7D%5EM">,则<!-- $(g \circ f)(x)=g(f(x))=g(Bx)=A(Bx)=(AB)x.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(g%20%5Ccirc%20f)(x)%3Dg(f(x))%3Dg(Bx)%3DA(Bx)%3D(AB)x.">     
>       + 转置(Transposition)：<!-- $A \in \mathbb{R}^{M \times N} \to A^T \in \mathbb{R}^{N \times M}, [A]_{mn}=[A^T]_{nm}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%20%5Cto%20A%5ET%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20M%7D%2C%20%5BA%5D_%7Bmn%7D%3D%5BA%5ET%5D_%7Bnm%7D.">
>       + 逐点乘积(Hadamard Product)：<!-- $A \in \mathbb{R}^{M \times N},B \in \mathbb{R}^{M \times N},A \bigodot B \in \mathbb{R}^{M \times N},[A \bigodot B]_{mn}=a_{mn}b_{mn},[cA]_{mn}=ca_{mn}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2CB%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2CA%20%5Cbigodot%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%5BA%20%5Cbigodot%20B%5D_%7Bmn%7D%3Da_%7Bmn%7Db_%7Bmn%7D%2C%5BcA%5D_%7Bmn%7D%3Dca_%7Bmn%7D."> 若矩阵A和B的维度不相等，则Hadamard积没有定义.
>      + Kronecker积(Kronecker Product)：<br/><!-- $A \in \mathbb{R}^{M \times N}, B \in \mathbb{R}^{S \times T},[A \bigotimes B]=\begin{bmatrix}a_{11}B & a_{12}B &\cdots & a_{1N}B\\a_{21}B & a_{22}B & \cdots & a_{2N}B\\ \vdots & \vdots & \ddots & \vdots\\a_{M1}B & a_{M2}B & \cdots & a_{MN}B\end{bmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BS%20%5Ctimes%20T%7D%2C%5BA%20%5Cbigotimes%20B%5D%3D%5Cbegin%7Bbmatrix%7Da_%7B11%7DB%20%26%20a_%7B12%7DB%20%26%5Ccdots%20%26%20a_%7B1N%7DB%5C%5Ca_%7B21%7DB%20%26%20a_%7B22%7DB%20%26%20%5Ccdots%20%26%20a_%7B2N%7DB%5C%5C%20%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%5C%5Ca_%7BM1%7DB%20%26%20a_%7BM2%7DB%20%26%20%5Ccdots%20%26%20a_%7BMN%7DB%5Cend%7Bbmatrix%7D.">适用于两个任意大小的矩阵相乘，且通常不符合交换律：<!-- $A \bigotimes B$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\KXlRfrqRcf.svg">不同于<!-- $B \bigotimes A$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\rZbiYhfiq6.svg">.
>      + 外积(Outer Product)：<!-- $a \in \mathbb{R}^M,b \in \mathbb{R}^N,a \bigotimes b=a \times b^T=\begin{bmatrix}a_1 \\ a_2 \\ \cdots \\ a_M\end{bmatrix}\begin{bmatrix}b_1 & b_2 & \cdots & b_N\end{bmatrix}=\begin{bmatrix}a_1b_1 & a_1b_2 & \cdots & a_1b_N\\a_2b_1 & a_2b_2 & \cdots & a_2b_N\\\vdots & \vdots & \ddots & \vdots\\a_Mb_1 & a_Mb_2 & \cdots & a_Mb_N\end{bmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%20%5Cin%20%5Cmathbb%7BR%7D%5EM%2Cb%20%5Cin%20%5Cmathbb%7BR%7D%5EN%2Ca%20%5Cbigotimes%20b%3Da%20%5Ctimes%20b%5ET%3D%5Cbegin%7Bbmatrix%7Da_1%20%5C%5C%20a_2%20%5C%5C%20%5Ccdots%20%5C%5C%20a_M%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Db_1%20%26%20b_2%20%26%20%5Ccdots%20%26%20b_N%5Cend%7Bbmatrix%7D%3D%5Cbegin%7Bbmatrix%7Da_1b_1%20%26%20a_1b_2%20%26%20%5Ccdots%20%26%20a_1b_N%5C%5Ca_2b_1%20%26%20a_2b_2%20%26%20%5Ccdots%20%26%20a_2b_N%5C%5C%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%5C%5Ca_Mb_1%20%26%20a_Mb_2%20%26%20%5Ccdots%20%26%20a_Mb_N%5Cend%7Bbmatrix%7D"> <br/><!-- $[a \bigotimes b]_{mn}=a_mb_n.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Ba%20%5Cbigotimes%20b%5D_%7Bmn%7D%3Da_mb_n."><br/>Kronecker积的特例(因此$\bigotimes$既可以表示Kronecker积也可以表示外积)，在线性代数中一般指*两个向量*的张量积，其结果为*一个矩阵*;与内积的区别在于内积结果为*标量*.
>      + 向量化(Vectorization)：<!-- $A=[a_{ij}]_{MN},vec(A)=[a_{11},a_{21},\cdots,a_{M1},a_{12},a_{22},\cdots,a_{M2},\cdots,a_{1N},a_{2N},\cdots,a_{MN}]^T.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\9tiRc8DZ3k.svg">矩阵的向量化是将一个矩阵表示为一个*列向量*.
>      + 迹：方块矩阵(<!-- $N \times N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=N%20%5Ctimes%20N">)A的对角线元素之和,<!-- $tr(A)=a_{11}+a_{22}+\cdots+a_{NN}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=tr(A)%3Da_%7B11%7D%2Ba_%7B22%7D%2B%5Ccdots%2Ba_%7BNN%7D.">矩阵乘法不满足交换律，但迹相同，<!-- $tr(AB)=tr(BA)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=tr(AB)%3Dtr(BA)">.
>      + 行列式(Determinant)：(几何意义)<br/>1. 从静态的体积概念来说是行列式中的行或列向量所构成的超平行多面体的有向面积或有向体积；<br/>2. 从动态的变换比例概念来说矩阵A的行列式det(A)就是线性变换A下的图形面积或体积(相对于单位面积或体积:正方形/正方体/超立方体,容积等于1）的**伸缩因子**(像域容积/原域容积)。
>        + 二阶行列式的几何性质(其行或列向量所张成的**有向**平行四边形面积)：
>           + <!-- $k\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}=\begin{vmatrix}ka_1&ka_2\\b_1&b_2\end{vmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\5Q3salT6v7.svg">.
>           + <!-- $\begin{vmatrix}a_1&a_2\\b_1+c_1&b_2+c_2\end{vmatrix}=\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}+\begin{vmatrix}a_1&a_2\\c_1&c_2\end{vmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\ilvDSpLabl.svg">.
>           + <!-- $\begin{vmatrix}a_1&a_2\\ka_1&ka_2\end{vmatrix}=0$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\Y2X28pRSiI.svg">.
>           + <!-- $\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}=-\begin{vmatrix}b_1&b_2\\a_1&a_2\end{vmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\gv3FjpXfzS.svg">.
>           + <!-- $\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}=\begin{vmatrix}a_1&b_1\\a_2&b_2\end{vmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\kIap9cHHJ0.svg">.
>        + 三阶行列式的几何性质(其行或列向量所张成的平行六面体的**有向**体积)：
>           + <!-- $det(a,b,c+d)=det(a,b,c)+det(a,b,d)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(a%2Cb%2Cc%2Bd)%3Ddet(a%2Cb%2Cc)%2Bdet(a%2Cb%2Cd)">
>           + <!-- $det(a,a,c)=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(a%2Ca%2Cc)%3D0"> <br/>(相当于三维空间中六面体被压成了高度为零的二维平面,这个平面的三维体积为零.)
>           + <!-- $det(a,b,c)=-det(b,a,c)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(a%2Cb%2Cc)%3D-det(b%2Ca%2Cc)">(改变方向.)
>           + <!-- $kdet(a,b,c)=det(ka,b,c)=det(a,kb,c)=det(a,b,kc)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=kdet(a%2Cb%2Cc)%3Ddet(ka%2Cb%2Cc)%3Ddet(a%2Ckb%2Cc)%3Ddet(a%2Cb%2Ckc)">
>           + <!-- $det(a,b,c+ka)=det(a,b,c)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(a%2Cb%2Cc%2Bka)%3Ddet(a%2Cb%2Cc)">
>           + <!-- $det A = det A'$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det%20A%20%3D%20det%20A'">
>        + 行列式的乘积项:由各个坐标轴上的有向线段所围起来的所有有向面积或有向体积的累加和。这个累加要注意每个面积或体积的方向或符号，方向相同的要加，方向相反的要减(叉积右手定则)，因而，这个累加的和是代数和.
>          + 二阶行列式的乘积项：(可以拆解成2个2维有向图形)<br/><!-- $\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}=\begin{vmatrix}a_1&0\\0&b_2\end{vmatrix}+\begin{vmatrix}0&a_2\\b_1&0\end{vmatrix}=a_1b_2-a_2b_1.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\cZY2Mhx7S8.svg">
>          + 三阶行列式的乘积项：(可以拆解成6个3维有向图形)<br/><!-- $\begin{vmatrix}a_1&a_2&a_3\\b_1&b_2&b_3\\c_1&c_2&c_3\end{vmatrix}=a_1\begin{vmatrix}b_2&b_3\\c_2&c_3\end{vmatrix}-a_2\begin{vmatrix}b_1&b_3\\c_1&c_3\end{vmatrix}+a_3\begin{vmatrix}b_1&b_2\\c_1&c_2\end{vmatrix}=a_1b_2c_3+a_2b_3c_1+a_3b_1c_2-a_1b_3c_2-a_2b_1c_3-a_3b_2c_1.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\jnKDgYqC4F.svg">
>          + n阶行列式的乘积项：(可以拆解成n!个n维有向图形)<br/><!-- $det(A)=\begin{vmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\a_{21}&a_{22}&\cdots&a_{2n}\\\vdots&\vdots&\ddots&\vdots\\a_{n1}&a_{n2}&\cdots&a_{nn}\end{vmatrix}=\sum_{j=1}^n(-1)^{i+j}A_{i,j}M_{i,j}(Laplace Expansion)=\sum_{(j_1,j_2,\cdots,j_n)}(-1)^{t}a_{1j_1}a_{2j_2}\cdots a_{nj_n}=\sum_{\sigma \in S_n}sgn(\sigma)\Pi_{i=1}^na_{i,\sigma(i)}.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\Rrw52HHcoC.svg"><br/>ps1: where <!-- $B_{i,j}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=B_%7Bi%2Cj%7D"> is the entry of the ith row and jth column of A, and <!-- $M_{i,j}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=M_%7Bi%2Cj%7D"> is the determinant of the submatrix obtained by removing the ith row and the jth column of A.The term <!-- $(-1)^{i+j}M_{i,j}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(-1)%5E%7Bi%2Bj%7DM_%7Bi%2Cj%7D"> is called the cofactor of <!-- $A_{i,j}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A_%7Bi%2Cj%7D"> in A.<br/>ps2: <!-- $sgn(\sigma)=(-1)^{N(\sigma)} \in \{-1,1\},N(\sigma)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=sgn(%5Csigma)%3D(-1)%5E%7BN(%5Csigma)%7D%20%5Cin%20%5C%7B-1%2C1%5C%7D%2CN(%5Csigma)">表示σ中反向对的个数，N(σ)=偶数则sgn(σ)=1,N(σ)=奇数则sgn(σ)=-1.<!-- $\sigma \in S_n, S_n=\{1,2,\cdots,n\}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Csigma%20%5Cin%20S_n%2C%20S_n%3D%5C%7B1%2C2%2C%5Ccdots%2Cn%5C%7D."><br/>\*行列式的几何意义(AndyJee, 2013)[[Blog]](https://www.cnblogs.com/AndyJee/p/3491487.html)<br/>\*线性代数的本质(3Blue1Brown,2017)[[Video]](https://www.bilibili.com/video/BV1ys411472E?p=8)<br/>\*Laplace Expansion[[Wiki]](https://en.wikipedia.org/wiki/Laplace_expansion)
>      + 秩(Rank)：means the number of dimensions in the output of a transformation, or the number of dimensions in the column space.如,2×2矩阵的秩最大为2，基向量依旧能张成整个二维空间且矩阵的行列式不为0；但对于3×3矩阵来说，秩=2意味着空间被压缩了,其行列式=0；当秩达到最大值时(<!-- $A \in \mathbb{R}^{M \times N},rank(A)=min(M,N)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2Crank(A)%3Dmin(M%2CN)">)，秩=列数(满秩)，<br/>\*线性代数的本质(3Blue1Brown,2017)[[Video]](https://www.bilibili.com/video/BV1ys411472E?p=8)
>      + (矩阵)范数：单个N维向量的<!-- $\mathcal{l}_p$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_p">范数<!-- $\to \mathcal{l}_p(\mathcal{v}) = \begin{Vmatrix} \mathcal{v} \end{Vmatrix}_p=(\sum_{n=1}^N\begin{vmatrix}v_n\end{vmatrix}^p)^{\frac 1p}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cto%20%5Cmathcal%7Bl%7D_p(%5Cmathcal%7Bv%7D)%20%3D%20%5Cbegin%7BVmatrix%7D%20%5Cmathcal%7Bv%7D%20%5Cend%7BVmatrix%7D_p%3D(%5Csum_%7Bn%3D1%7D%5EN%5Cbegin%7Bvmatrix%7Dv_n%5Cend%7Bvmatrix%7D%5Ep)%5E%7B%5Cfrac%201p%7D"><br/>矩阵<!-- $A \in \mathbb{R}^{M \times N}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D">的<!-- $\mathcal{l}_p$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_p">范数<!-- $\to \begin{Vmatrix}A\end{Vmatrix}_p=(\sum_{m=1}^M \sum_{n=1}^N \begin{vmatrix}a_{mn}\end{vmatrix}^p)^{\frac 1p}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cto%20%5Cbegin%7BVmatrix%7DA%5Cend%7BVmatrix%7D_p%3D(%5Csum_%7Bm%3D1%7D%5EM%20%5Csum_%7Bn%3D1%7D%5EN%20%5Cbegin%7Bvmatrix%7Da_%7Bmn%7D%5Cend%7Bvmatrix%7D%5Ep)%5E%7B%5Cfrac%201p%7D.">
>   + 矩阵类型：
>      + 对称矩阵(Symmetric Matrix)：$A=A^T$,其转置等于自己的矩阵.实系数矩阵与自己的转置相乘后是实对称矩阵，而且实对称矩阵必定可以分解出特征值与特征向量。
>      + 对角矩阵(Diagonal Matrix)：<!-- $[A]_{mn}=0,\forall m,n \in [1,\cdots,N],and m \neq n.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5BA%5D_%7Bmn%7D%3D0%2C%5Cforall%20m%2Cn%20%5Cin%20%5B1%2C%5Ccdots%2CN%5D%2Cand%20m%20%5Cneq%20n.">除主对角线之外的元素都为0的矩阵，通常指方块矩阵.
>      + 单位矩阵(Identity Matrix)：<br/><!-- $I_N=diag(1,1,\cdots,1)=\begin{bmatrix}1&0&\cdots&0\\0&1&\cdots&0\\\vdots&\vdots&\ddots&0\\0&0&\cdots&1\end{bmatrix},AI_N=I_MA=A(A \in \mathbb{R}^{M \times M}).$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=I_N%3Ddiag(1%2C1%2C%5Ccdots%2C1)%3D%5Cbegin%7Bbmatrix%7D1%260%26%5Ccdots%260%5C%5C0%261%26%5Ccdots%260%5C%5C%5Cvdots%26%5Cvdots%26%5Cddots%260%5C%5C0%260%26%5Ccdots%261%5Cend%7Bbmatrix%7D%2CAI_N%3DI_MA%3DA(A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20M%7D)."><br/>特殊的对角矩阵，其主对角线元素为1，其余为0.
>      + 逆矩阵(Inverse Matrix)：<br/><!-- $det(A) \neq 0, then A^{-1} exist. A \in \mathbb{R}^{N \times N},AA^{-1}=I_N.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(A)%20%5Cneq%200%2C%20then%20A%5E%7B-1%7D%20exist.%20A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20N%7D%2CAA%5E%7B-1%7D%3DI_N.">如果存在另一个方块矩阵B使<!-- $AB=BA=I_N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=AB%3DBA%3DI_N">,则称A是*可逆的*，矩阵B称为A的逆矩阵，记作<!-- $A^{-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%5E%7B-1%7D">.
>      + 正定矩阵(Positive-Definite Matrix)：
>      + 正交矩阵(Orthogonal Matrix)：<!-- $A \in \mathbb{R}^{N \times N},A^T=A^{-1}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20N%7D%2CA%5ET%3DA%5E%7B-1%7D.">正交矩阵A的转置矩阵等于其逆矩阵.如果矩阵是单位正交矩阵，<!-- $A \times A^T=A \times A^{-1}=I$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Ctimes%20A%5ET%3DA%20%5Ctimes%20A%5E%7B-1%7D%3DI">.
>      + Gram矩阵(Gram Matrix)：
>   + 特征值与特征向量(Eigenvalue & Eigenvector)：<!-- $A\ \in \mathbb{R}^{N \times N},A \vec{v}=\lambda \vec{v}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%5C%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20N%7D%2CA%20%5Cvec%7Bv%7D%3D%5Clambda%20%5Cvec%7Bv%7D."><br/>特征向量<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">在矩阵A的作用下，保持方向不变进行比例为λ的伸缩.从$A \vec{v}$相对于<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">伸缩可以看出，特征向量分别对应特征值λ＞1或λ＜1.<br/>特征值λ和特征向量<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">求解示例：<br/><!-- $Ax={\lambda x \Rightarrow Ax=\lambda Ex \Rightarrow (\lambda E-A)x}=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=Ax%3D%7B%5Clambda%20x%20%5CRightarrow%20Ax%3D%5Clambda%20Ex%20%5CRightarrow%20(%5Clambda%20E-A)x%7D%3D0"><br/><!-- $A=\begin{bmatrix}e&2&-5\\6&4&-9\\5&3&-7\end{bmatrix},|\lambda E-A|=\begin{vmatrix}\lambda -4&-2&5\\-6&\lambda-4&9\\-5&-3&\lambda+7\end{vmatrix}=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3D%5Cbegin%7Bbmatrix%7De%262%26-5%5C%5C6%264%26-9%5C%5C5%263%26-7%5Cend%7Bbmatrix%7D%2C%7C%5Clambda%20E-A%7C%3D%5Cbegin%7Bvmatrix%7D%5Clambda%20-4%26-2%265%5C%5C-6%26%5Clambda-4%269%5C%5C-5%26-3%26%5Clambda%2B7%5Cend%7Bvmatrix%7D%3D0"><br/><!-- $\Rightarrow \lambda^2(\lambda-1)=0,\lambda_1=1,\lambda_2=\lambda_3=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow%20%5Clambda%5E2(%5Clambda-1)%3D0%2C%5Clambda_1%3D1%2C%5Clambda_2%3D%5Clambda_3%3D0"><br/><!-- $\lambda_1=1 \Rightarrow (E-A)x=0,E-A={\begin{bmatrix}-3&-2&5\\-6&-3&9\\-5&-3&8\end{bmatrix}}\Rightarrow{\begin{bmatrix}1&0&-1\\0&1&-1\\0&0&0\end{bmatrix}}\Rightarrow{(E-A)x=\begin{bmatrix}1&0&-1\\0&1&-1\\0&0&0\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}}=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda_1%3D1%20%5CRightarrow%20(E-A)x%3D0%2CE-A%3D%7B%5Cbegin%7Bbmatrix%7D-3%26-2%265%5C%5C-6%26-3%269%5C%5C-5%26-3%268%5Cend%7Bbmatrix%7D%7D%5CRightarrow%7B%5Cbegin%7Bbmatrix%7D1%260%26-1%5C%5C0%261%26-1%5C%5C0%260%260%5Cend%7Bbmatrix%7D%7D%5CRightarrow%7B(E-A)x%3D%5Cbegin%7Bbmatrix%7D1%260%26-1%5C%5C0%261%26-1%5C%5C0%260%260%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%5C%5Cx_2%5C%5Cx_3%5Cend%7Bbmatrix%7D%7D%3D0"><br/><!-- $\lambda_2=\lambda_3=0 \Rightarrow (Similar)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda_2%3D%5Clambda_3%3D0%20%5CRightarrow%20(Similar)"><br/>令<!-- $x_1=1,\vec{v_1}=\begin{bmatrix}1\\1\\1\end{bmatrix},\vec{v_2}=\vec{v_3}=\begin{bmatrix}1\\3\\2\end{bmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x_1%3D1%2C%5Cvec%7Bv_1%7D%3D%5Cbegin%7Bbmatrix%7D1%5C%5C1%5C%5C1%5Cend%7Bbmatrix%7D%2C%5Cvec%7Bv_2%7D%3D%5Cvec%7Bv_3%7D%3D%5Cbegin%7Bbmatrix%7D1%5C%5C3%5C%5C2%5Cend%7Bbmatrix%7D.">
>   + 矩阵分解(Matrix Decomposition|Matrix Factorization)：
>      + 相似矩阵：同一线性变换，不同基下的矩阵称为相似矩阵.
>        |步骤||
>        |:-----|:------|
>        |<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">是<!-- $V_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=V_2">空间下的点|<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">|
>        |<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">通过可逆矩阵P变为<!-- $V_1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=V_1">空间下的点|<!-- $P\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P%5Cvec%7Bv%7D">|
>        |在<!-- $V_1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=V_1">下通过特征矩阵A完成线性变换|<!-- $AP\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=AP%5Cvec%7Bv%7D">|
>        |通过<!-- $P^{-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P%5E%7B-1%7D">变回<!-- $V_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=V_2">空间下的点|<!-- $P^{-1}AP\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P%5E%7B-1%7DAP%5Cvec%7Bv%7D">|
>        |A、B均为n阶矩阵，若存在可逆矩阵P使<!-- $B=P^{-1}AP$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=B%3DP%5E%7B-1%7DAP">，则A、B为相似矩阵|<!-- $B\vec{v}=P^{-1}AP\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=B%5Cvec%7Bv%7D%3DP%5E%7B-1%7DAP%5Cvec%7Bv%7D">|
>      + 特征分解(Eigendecomposition)：矩阵A可以对角化的话(方块矩阵)，可以通过相似矩阵进行特征值分解：<br/><!-- $A=P\Lambda P^{-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3DP%5CLambda%20P%5E%7B-1%7D">，<br/><!-- $A=\begin{bmatrix}2&-1\\-1&2\end{bmatrix}={\begin{bmatrix}-\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}\\\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}\end{bmatrix}}{\begin{bmatrix}3&0\\0&1\end{bmatrix}}{\begin{bmatrix}-\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}\\\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}\end{bmatrix}}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3D%5Cbegin%7Bbmatrix%7D2%26-1%5C%5C-1%262%5Cend%7Bbmatrix%7D%3D%7B%5Cbegin%7Bbmatrix%7D-%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%26%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%5C%5C%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%26%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%5Cend%7Bbmatrix%7D%7D%7B%5Cbegin%7Bbmatrix%7D3%260%5C%5C0%261%5Cend%7Bbmatrix%7D%7D%7B%5Cbegin%7Bbmatrix%7D-%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%26%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%5C%5C%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%26%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%5Cend%7Bbmatrix%7D%7D">，其中<!-- $\Lambda$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CLambda">为对角矩阵，P的列向量是单位特征向量且相互正交(一个向量乘以正交矩阵的几何意义就是把该向量从当前坐标系变换到这个矩阵所表示的坐标系).<br/>对于方阵而言，矩阵不会进行维度的升降，所以矩阵代表的运动实际上只有两种：旋转和拉伸.因此特征值分解实际上是把运动分解，即A方阵既有旋转也有拉伸，特征分解后正交矩阵P与其逆矩阵<!-- $P^{-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P%5E%7B-1%7D">只有旋转，对角矩阵<!-- $\Lambda$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CLambda">只有拉伸.
>      + 奇异值分解(Sigular Value Decomposition, SVD)：在实际应用中，特征向量很可能不是正交的，则需要用到奇异值分解.奇异值分解就是把矩阵分成多个"分力"；奇异值的大小就是各个"分力"的大小;奇异值分解实际上把矩阵变换分成了三部分：旋转、拉伸和投影.<br/><!-- $A=U \Sigma V^T$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3DU%20%5CSigma%20V%5ET">,A为<!-- $M \times N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=M%20%5Ctimes%20N">矩阵，U为<!-- $M \times M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=M%20%5Ctimes%20M">正交矩阵，∑为<!-- $M \times N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=M%20%5Ctimes%20N">矩形对角矩阵，V为<!-- $N \times N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=N%20%5Ctimes%20N">正交矩阵.<br/>A的非零奇异值为<!-- $AA^T$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=AA%5ET">或<!-- $A^T A$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%5ET%20A">的非零特征值的平方根:<br/><!-- $A=U \Sigma V^T \Leftrightarrow \begin{bmatrix}1&1\\0&1\\1&0\end{bmatrix}={\begin{bmatrix}\frac{2}{\sqrt{6}}&0&-\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{6}}&-\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{6}}&\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{3}}\end{bmatrix}}{\begin{bmatrix}\sqrt{3}&0\\0&1\\0&0\end{bmatrix}}{\begin{bmatrix}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\end{bmatrix}}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3DU%20%5CSigma%20V%5ET%20%5CLeftrightarrow%20%5Cbegin%7Bbmatrix%7D1%261%5C%5C0%261%5C%5C1%260%5Cend%7Bbmatrix%7D%3D%7B%5Cbegin%7Bbmatrix%7D%5Cfrac%7B2%7D%7B%5Csqrt%7B6%7D%7D%260%26-%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%26-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%5Cend%7Bbmatrix%7D%7D%7B%5Cbegin%7Bbmatrix%7D%5Csqrt%7B3%7D%260%5C%5C0%261%5C%5C0%260%5Cend%7Bbmatrix%7D%7D%7B%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cend%7Bbmatrix%7D%7D"><br/><!-- $A^TA=(V \Sigma^T U^T)(U \Sigma V^T)=V \Sigma^2 V^T$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%5ETA%3D(V%20%5CSigma%5ET%20U%5ET)(U%20%5CSigma%20V%5ET)%3DV%20%5CSigma%5E2%20V%5ET"><br/><!-- $\begin{bmatrix}1&0&1\\1&1&0\end{bmatrix}\begin{bmatrix}1&1\\0&1\\1&0\end{bmatrix}=\begin{bmatrix}2&1\\1&2\end{bmatrix}\Rightarrow \lambda_1=3,\lambda_2=1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bbmatrix%7D1%260%261%5C%5C1%261%260%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D1%261%5C%5C0%261%5C%5C1%260%5Cend%7Bbmatrix%7D%3D%5Cbegin%7Bbmatrix%7D2%261%5C%5C1%262%5Cend%7Bbmatrix%7D%5CRightarrow%20%5Clambda_1%3D3%2C%5Clambda_2%3D1"><br/><!-- $\Rightarrow \vec{v_1}=\begin{bmatrix}\frac{1}{\sqrt 2}\\\frac{1}{\sqrt 2}\end{bmatrix},\vec{v}_2=\begin{bmatrix}\frac{1}{\sqrt 2}\\-\frac{1}{\sqrt 2}\end{bmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow%20%5Cvec%7Bv_1%7D%3D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D%2C%5Cvec%7Bv%7D_2%3D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5C%5C-%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D"><br/><!-- $AA^T=(U\Sigma V^T)(V \Sigma^T U^T)=U \Sigma^2 U^T$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=AA%5ET%3D(U%5CSigma%20V%5ET)(V%20%5CSigma%5ET%20U%5ET)%3DU%20%5CSigma%5E2%20U%5ET"><br/><!-- $\Rightarrow \begin{bmatrix}1&1\\0&1\\1&0\end{bmatrix}\begin{bmatrix}1&0&1\\1&1&0\end{bmatrix}=\begin{bmatrix}2&1&1\\1&1&0\\1&0&1\end{bmatrix}\Rightarrow \lambda_1=3,\lambda_2=1,\lambda_3=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow%20%5Cbegin%7Bbmatrix%7D1%261%5C%5C0%261%5C%5C1%260%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D1%260%261%5C%5C1%261%260%5Cend%7Bbmatrix%7D%3D%5Cbegin%7Bbmatrix%7D2%261%261%5C%5C1%261%260%5C%5C1%260%261%5Cend%7Bbmatrix%7D%5CRightarrow%20%5Clambda_1%3D3%2C%5Clambda_2%3D1%2C%5Clambda_3%3D0"><br/><!-- $\Rightarrow \vec{v_1}=\begin{bmatrix}\frac{2}{\sqrt 6}\\\frac{1}{\sqrt 6}\\\frac{1}{\sqrt 6}\end{bmatrix},\vec{v}_2=\begin{bmatrix}0\\-\frac{1}{\sqrt 2}\\\frac{1}{\sqrt 2}\end{bmatrix},\vec{v}_3=\begin{bmatrix}-\frac{1}{\sqrt 3}\\\frac{1}{\sqrt 3}\\\frac{1}{\sqrt 3}\end{bmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow%20%5Cvec%7Bv_1%7D%3D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B2%7D%7B%5Csqrt%206%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%206%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%206%7D%5Cend%7Bbmatrix%7D%2C%5Cvec%7Bv%7D_2%3D%5Cbegin%7Bbmatrix%7D0%5C%5C-%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D%2C%5Cvec%7Bv%7D_3%3D%5Cbegin%7Bbmatrix%7D-%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5Cend%7Bbmatrix%7D"><br/><!-- $A=\lambda_1 \vec{u}_1 \vec{v}_1^T+\lambda_2 \vec{u}_2 \vec{v}_2^T=\sqrt{3}\begin{bmatrix}\frac{2}{\sqrt 6}\\\frac{1}{\sqrt 6}\\\frac{1}{\sqrt 6}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt 2}&\frac{1}{\sqrt 2}\end{bmatrix}+1\begin{bmatrix}-\frac{1}{\sqrt 3}\\\frac{1}{\sqrt 3}\\\frac{1}{\sqrt 3}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt 2}&-\frac{1}{\sqrt 2}\end{bmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3D%5Clambda_1%20%5Cvec%7Bu%7D_1%20%5Cvec%7Bv%7D_1%5ET%2B%5Clambda_2%20%5Cvec%7Bu%7D_2%20%5Cvec%7Bv%7D_2%5ET%3D%5Csqrt%7B3%7D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B2%7D%7B%5Csqrt%206%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%206%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%206%7D%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D%2B1%5Cbegin%7Bbmatrix%7D-%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%26-%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D."><br/>
       \* 线性代数(马同学)[[Blog]](https://www.matongxue.com/madocs/228/)<br/>
       \* 奇异值分解(渺然如尘)[[Video]](https://www.bilibili.com/video/BV1mx411E74T?p=4)
> ----
> #### Appendix.B 微积分(Calculus)
> + 微分(Differentiation)
>    + 导数(Derivative)：定义一个函数<!-- $f:\mathbb{R} \to \mathbb{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f%3A%5Cmathbb%7BR%7D%20%5Cto%20%5Cmathbb%7BR%7D"> (定义域和值域都是实数域)，若<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">在点<!-- $x_0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x_0">的某个邻域<!-- $\Delta x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CDelta%20x">内，极限<!-- $f'(x_0)=lim_{\Delta x \to 0}{{f(x_o+\Delta x)-f(x_0)}\over{\Delta x}}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f'(x_0)%3Dlim_%7B%5CDelta%20x%20%5Cto%200%7D%7B%7Bf(x_o%2B%5CDelta%20x)-f(x_0)%7D%5Cover%7B%5CDelta%20x%7D%7D">存在，则称函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">在点<!-- $x_0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x_0">可导，<!-- $f'(x_0)=\frac{df(x_0)}{dx}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f'(x_0)%3D%5Cfrac%7Bdf(x_0)%7D%7Bdx%7D">成为其导数.
>       + 常见函数的导数：
>         |函数    |函数形式               |导数            |
>         |:------|:----------------------|:---------------|
>         |常函数  |<!-- $f(x)=C$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)%3DC">,C为常数.      |<!-- $f'(x)=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f'(x)%3D0">       |
>         |幂函数  |<!-- $f(x)=x^r$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)%3Dx%5Er">,r为非零实数.|<!-- $f'(x)=rx^{r-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f'(x)%3Drx%5E%7Br-1%7D">|
>         |指数函数|<!-- $f(x)=exp(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)%3Dexp(x)">          |<!-- $f'(x)=exp(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f'(x)%3Dexp(x)">  |
>         |对数函数|<!-- $f(x)=log(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)%3Dlog(x)">          |<!-- $f'(x)=\frac 1x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f'(x)%3D%5Cfrac%201x">|  
>       + 高阶导数：对一个函数的导数继续求导. 例，二阶导数<!-- $f''(x)=f^{(2)}(x)={{d^{(2)}f(x)}\over{dx^2}}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f''(x)%3Df%5E%7B(2)%7D(x)%3D%7B%7Bd%5E%7B(2)%7Df(x)%7D%5Cover%7Bdx%5E2%7D%7D">.
>       + 偏导数(Partial Derivative)：多元变量函数<!-- $f:\mathbb{R}^D \to \mathbb{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f%3A%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D">关于其中一个变量<!-- $x_i$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x_i">的导数，保持其他变量固定.例，一阶偏导<!-- $f_{x_i}'(x)=\nabla_{x_i}f(x)={{\partial f(x)} \over {\partial x_i}}={{\partial} \over {\partial x_i}}f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f_%7Bx_i%7D'(x)%3D%5Cnabla_%7Bx_i%7Df(x)%3D%7B%7B%5Cpartial%20f(x)%7D%20%5Cover%20%7B%5Cpartial%20x_i%7D%7D%3D%7B%7B%5Cpartial%7D%20%5Cover%20%7B%5Cpartial%20x_i%7D%7Df(x)">.
>    + 微分及可微函数(Differentiation & Differentiable Function)：给定一个连续函数，计算其导数的过程称为*微分*. 如果一个函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">在定义域内的所有点都存在导数，则<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">称为*可微函数*. 可微函数<!-- $\Rightarrow$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow">连续函数，连续函数则不一定可微. 例，函数<!-- $f(x)=\begin{vmatrix}x\end{vmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)%3D%5Cbegin%7Bvmatrix%7Dx%5Cend%7Bvmatrix%7D">为连续函数，但在点<!-- $x=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%3D0">处不可导.
>    + 泰勒公式(Taylor's Formula)：
>       + 如果函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">在点<!-- $a$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a">处<!-- $n$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=n">次可导<!-- $(n \ge 1)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(n%20%5Cge%201)">，在一个包含点<!-- $a$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a">的区间上的任意<!-- $x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x">都有<!-- $f(x)=f(a)+\frac{1}{1!}f'(a)(x-a)+\frac{1}{2!}f^{(2)}(a)(x-a)^2+..._\frac{1}{n!}f^{(n)}(a)(x-a)^n+R_n(x), n\ge 1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)%3Df(a)%2B%5Cfrac%7B1%7D%7B1!%7Df'(a)(x-a)%2B%5Cfrac%7B1%7D%7B2!%7Df%5E%7B(2)%7D(a)(x-a)%5E2%2B..._%5Cfrac%7B1%7D%7Bn!%7Df%5E%7B(n)%7D(a)(x-a)%5En%2BR_n(x)%2C%20n%5Cge%201">.
>       + 泰勒公式也是用来求解函数在特定点的邻域中的近似值，一阶泰勒展开可以视为变化，二阶视为变化的变化，依次求和所有变化，原文具体表述是‘已知函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">在点<!-- $a$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a">的各阶导数值，用这些导数值做系数构建一个多项式来近似该函数在点$a$的邻域中的值. 多项式部分称为函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">在点<!-- $a$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a">的<!-- $n$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=n">阶泰勒展开式，<!-- $R_n(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=R_n(x)">称为泰勒公式的余项，是<!-- $(x-a)^n$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(x-a)%5En">的高阶无穷小’. 
> + 积分(Integration)：积分是微分的逆过程. 微分是对连续函数求导数的过程，积分是从导数推算出原函数的过程.
>    + 不定积分(Indefinite Integral)：
>         + <!-- $F(x)=\int f(x)dx$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=F(x)%3D%5Cint%20f(x)dx">,<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">是<!-- $F(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=F(x)">的导数，<!-- $F(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=F(x)">是<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">的原函数、不定积分，<!-- $dx$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=dx">表示积分变量是<!-- $x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x">. 
>         + 函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">的不定积分是不唯一的，根据导数性质，<!-- $F(x)+C$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=F(x)%2BC">也是<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">的不定积分(<!-- $C$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=C">为常数).
>    + 定积分(Definite Integral)：
>       + <!-- $F(x)=\int_b^a f(x)dx,x \in [a,b]$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=F(x)%3D%5Cint_b%5Ea%20f(x)dx%2Cx%20%5Cin%20%5Ba%2Cb%5D">. <!-- $F(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=F(x)">是<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">的定积分.
>       + 可以理解为面积(在坐标平面上由函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">，垂直直线<!-- $x=a,x=b, x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%3Da%2Cx%3Db%2C%20x">轴围起来的区域的带符号的面积，<!-- $x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x">轴以上的面积为正值，<!-- $x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x">轴以下的面积为负值).
>       + 黎曼积分(Riemann Integral)：
>          + 对于闭区间<!-- $[a,b]$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Ba%2Cb%5D">，定义<!-- $[a,b]$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Ba%2Cb%5D">的一个分割为在此区间中取一个有限的点列<!-- $a=x_0 \lt x_1 \lt x_2 \lt ... \lt x_N=b.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%3Dx_0%20%5Clt%20x_1%20%5Clt%20x_2%20%5Clt%20...%20%5Clt%20x_N%3Db."> 这些点将区间<!-- $[a,b]$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Ba%2Cb%5D"> 分割为N个子区间<!-- $[x_{n-1},x_n],1 \le n \le N$，每个子区间取出一个点$t_n \in [x_{n-1},x_n]$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Bx_%7Bn-1%7D%2Cx_n%5D%2C1%20%5Cle%20n%20%5Cle%20N%24%EF%BC%8C%E6%AF%8F%E4%B8%AA%E5%AD%90%E5%8C%BA%E9%97%B4%E5%8F%96%E5%87%BA%E4%B8%80%E4%B8%AA%E7%82%B9%24t_n%20%5Cin%20%5Bx_%7Bn-1%7D%2Cx_n%5D">作为代表，函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">在这个分割上的黎曼和为<!-- $\sum_{n=1}^Nf(t_n)(x_n-x_{n-1}).$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Csum_%7Bn%3D1%7D%5ENf(t_n)(x_n-x_%7Bn-1%7D).">
>          + 不同分割的黎曼和不同.当<!-- $\lambda=max_{n=1}^N(x_n-x_{n-1})$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda%3Dmax_%7Bn%3D1%7D%5EN(x_n-x_%7Bn-1%7D)">足够小时，如果所有的黎曼和都趋于某个极限，那么这个极限叫做函数$f(x)$在闭区间<!-- $[a,b]$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Ba%2Cb%5D">上的黎曼积分.
> + 矩阵微积分(Matrix Calculus)：用矩阵和向量来表示多元函数偏导数. 有2种表示方式：分子布局(Numerator Layout)和分母布局(Denominator Layout).
>    |表示类型|标量关于向量的偏导数|向量关于标量的偏导数|向量关于向量的偏导数|
>    |:------:|:------------------:|:-----------------:|:-----------------:|
>    |<!-- $x,y$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%2Cy">|标量<!-- $y=f(x)\in\mathbb{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=y%3Df(x)%5Cin%5Cmathbb%7BR%7D">，<br/>向量<!-- $x\in \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%5Cin%20%5Cmathbb%7BR%7D%5EM">.|向量<!-- $y=f(x)\in \mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=y%3Df(x)%5Cin%20%5Cmathbb%7BR%7D%5EN">，<br/>标量<!-- $x\in \mathbb{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%5Cin%20%5Cmathbb%7BR%7D">.|向量<!-- $y=f(x)\in \mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=y%3Df(x)%5Cin%20%5Cmathbb%7BR%7D%5EN">，<br/>向量<!-- $x\in \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%5Cin%20%5Cmathbb%7BR%7D%5EM">.|
>    |分子布局|<!-- ${{\partial y} \over {\partial x}}=[{{\partial y} \over {\partial x_1}},...,{{\partial y} \over {\partial x_M}}]\in\mathbb{R}^{1\times M}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3D%5B%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x_1%7D%7D%2C...%2C%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x_M%7D%7D%5D%5Cin%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%20M%7D"><br/>(<!-- ${\partial y} \over {\partial x}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D">为行向量)|<!-- ${{\partial y} \over {\partial x}}=[{{\partial y_1} \over {\partial x}},...,{{\partial y_N} \over {\partial x}}]\in \mathbb{R}^{1\times N}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3D%5B%7B%7B%5Cpartial%20y_1%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%2C...%2C%7B%7B%5Cpartial%20y_N%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%5D%5Cin%20%5Cmathbb%7BR%7D%5E%7B1%5Ctimes%20N%7D">|-|
>    |分母布局<br/>(默认)|<!-- ${\partial y \over \partial x}=[{\partial y \over \partial x_1},...,{\partial y \over \partial x_M}]^T\in\mathbb{R}^{M\times1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%5Cpartial%20y%20%5Cover%20%5Cpartial%20x%7D%3D%5B%7B%5Cpartial%20y%20%5Cover%20%5Cpartial%20x_1%7D%2C...%2C%7B%5Cpartial%20y%20%5Cover%20%5Cpartial%20x_M%7D%5D%5ET%5Cin%5Cmathbb%7BR%7D%5E%7BM%5Ctimes1%7D"><br/>(<!-- $\partial y \over \partial x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cpartial%20y%20%5Cover%20%5Cpartial%20x">为列向量)|<!-- ${\partial y \over \partial x}=[{\partial y_1 \over \partial x},...,{\partial y_N \over \partial x}]\in \mathbb{R}^{N \times 1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%5Cpartial%20y%20%5Cover%20%5Cpartial%20x%7D%3D%5B%7B%5Cpartial%20y_1%20%5Cover%20%5Cpartial%20x%7D%2C...%2C%7B%5Cpartial%20y_N%20%5Cover%20%5Cpartial%20x%7D%5D%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%201%7D">|<!-- ${{\partial f(x)} \over {\partial x}}=\begin{bmatrix}{\partial y_1} \over {\partial x_1} & ... & {\partial y_N} \over {\partial x_1}\\ \vdots & \ddots & \vdots\\{\partial y_1} \over {\partial x_M} & ... & {\partial y_N} \over {\partial x_M} \end{bmatrix}\in \mathbb{R}^{M \times N}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial%20f(x)%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3D%5Cbegin%7Bbmatrix%7D%7B%5Cpartial%20y_1%7D%20%5Cover%20%7B%5Cpartial%20x_1%7D%20%26%20...%20%26%20%7B%5Cpartial%20y_N%7D%20%5Cover%20%7B%5Cpartial%20x_1%7D%5C%5C%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%5C%5C%7B%5Cpartial%20y_1%7D%20%5Cover%20%7B%5Cpartial%20x_M%7D%20%26%20...%20%26%20%7B%5Cpartial%20y_N%7D%20%5Cover%20%7B%5Cpartial%20x_M%7D%20%5Cend%7Bbmatrix%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D"><br/>(函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">的Jacobian Matrix的转置.)|
>    + 函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">关于<!-- $x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x">的二阶偏导数: 向量<!-- $x\in \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%5Cin%20%5Cmathbb%7BR%7D%5EM">，函数<!-- $f(x)\in \mathbb{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)%5Cin%20%5Cmathbb%7BR%7D">，<br/><!-- $H=\nabla^2 f(x)={{\partial^2 f(x)} \over {\partial x^2}}=\begin{bmatrix}{\partial^2 y} \over {\partial x_1^2} & \cdots & {\partial^2 y} \over {\partial x_1 x_M}\\ \vdots & \ddots & \vdots\\ {\partial^2 y} \over {\partial x_M x_1} & \cdots & {\partial^2 y} \over {\partial x_M x_M}\end{bmatrix} \in \mathbb{R}^{M \times M}$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\I2zwP4YeP4.svg"><br/>(函数<!-- $f(x)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(x)">的Hessian矩阵).
>    + 导数法则：
>       + 加减法则：若<!-- $x \in \mathbb{R}^M,y=f(x) \in \mathbb{R}^N,z=g(x) \in \mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%20%5Cin%20%5Cmathbb%7BR%7D%5EM%2Cy%3Df(x)%20%5Cin%20%5Cmathbb%7BR%7D%5EN%2Cz%3Dg(x)%20%5Cin%20%5Cmathbb%7BR%7D%5EN">，则<br/><!-- ${{\partial(y+z)} \over {\partial x}}={{\partial y} \over {\partial x}}+{{\partial z} \over {\partial x}}\in \mathbb{R}^{M \times N}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial(y%2Bz)%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3D%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%2B%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D">.
>       + 乘法法则：
>          + 若<!-- $x \in \mathbb{R}^M,y=f(x) \in \mathbb{R}^N,z=g(x) \in \mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%20%5Cin%20%5Cmathbb%7BR%7D%5EM%2Cy%3Df(x)%20%5Cin%20%5Cmathbb%7BR%7D%5EN%2Cz%3Dg(x)%20%5Cin%20%5Cmathbb%7BR%7D%5EN">，则<br/><!-- ${{\partial(y^Tz)} \over {\partial x}}={{\partial y} \over {\partial x}}z+{{\partial z} \over {\partial x}}y \in \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial(y%5ETz)%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3D%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7Dz%2B%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7Dy%20%5Cin%20%5Cmathbb%7BR%7D%5EM">.
>          + 若<!-- $x \in \mathbb{R}^M,y=f(x) \in \mathbb{R}^S,z=g(x) \in \mathbb{R}^T,A \in \mathbb{R}^{S \times T}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%20%5Cin%20%5Cmathbb%7BR%7D%5EM%2Cy%3Df(x)%20%5Cin%20%5Cmathbb%7BR%7D%5ES%2Cz%3Dg(x)%20%5Cin%20%5Cmathbb%7BR%7D%5ET%2CA%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BS%20%5Ctimes%20T%7D">且和<!-- $x$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x">无关，则<br/><!-- ${{\partial y^{T}Az} \over {\partial x}}={{\partial y} \over {\partial x}}Az+{{\partial z} \over {\partial x}}A^Ty \in \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial%20y%5E%7BT%7DAz%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3D%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7DAz%2B%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7DA%5ETy%20%5Cin%20%5Cmathbb%7BR%7D%5EM">.
>          + 若<!-- $x \in \mathbb{R}^M,y=f(x) \in \mathbb{R},z=g(x) \in \mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%20%5Cin%20%5Cmathbb%7BR%7D%5EM%2Cy%3Df(x)%20%5Cin%20%5Cmathbb%7BR%7D%2Cz%3Dg(x)%20%5Cin%20%5Cmathbb%7BR%7D%5EN">，则<br/><!-- ${{\partial yz} \over {\partial x}}=y{{\partial z} \over {\partial x}}+{{\partial y} \over {\partial x}}z^T \in \mathbb{R}^{M \times N}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial%20yz%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3Dy%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%2B%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7Dz%5ET%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D">.
>       + 链式法则(Chain Rule)：
>          + 若<!-- $x \in \mathbb{R},y=g(x) \in \mathbb{R}^M,z=f(y) \in \mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%20%5Cin%20%5Cmathbb%7BR%7D%2Cy%3Dg(x)%20%5Cin%20%5Cmathbb%7BR%7D%5EM%2Cz%3Df(y)%20%5Cin%20%5Cmathbb%7BR%7D%5EN">，则<br/><!-- ${{\partial z} \over {\partial x}}={{\partial y} \over {\partial x}}{{\partial z} \over {\partial y}}\in \mathbb{R}^{1 \times N}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3D%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20y%7D%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B1%20%5Ctimes%20N%7D">.
>          + 若<!-- $x \in \mathbb{R}^M,y=g(x) \in \mathbb{R}^K,z=f(y) \in \mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%20%5Cin%20%5Cmathbb%7BR%7D%5EM%2Cy%3Dg(x)%20%5Cin%20%5Cmathbb%7BR%7D%5EK%2Cz%3Df(y)%20%5Cin%20%5Cmathbb%7BR%7D%5EN">，则<!-- ${{\partial z} \over {\partial x}}={{\partial y} \over {\partial x}}{{\partial z} \over {\partial y}}\in \mathbb{R}^{M \times N}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%3D%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x%7D%7D%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20y%7D%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D"><br/>
>          + 若<!-- $x \in \mathbb{R}^{M \times N},y=g(x) \in \mathbb{R}^K,z=f(y) \in \mathbb{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2Cy%3Dg(x)%20%5Cin%20%5Cmathbb%7BR%7D%5EK%2Cz%3Df(y)%20%5Cin%20%5Cmathbb%7BR%7D">，则<!-- ${{\partial z} \over {\partial x_{ij}}}={{\partial y} \over {\partial x_{ij}}}{{\partial z} \over {\partial y}}\in \mathbb{R}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20x_%7Bij%7D%7D%7D%3D%7B%7B%5Cpartial%20y%7D%20%5Cover%20%7B%5Cpartial%20x_%7Bij%7D%7D%7D%7B%7B%5Cpartial%20z%7D%20%5Cover%20%7B%5Cpartial%20y%7D%7D%5Cin%20%5Cmathbb%7BR%7D."><br/>
> + 常见函数导数
>    + 向量函数及其导数:
>       + ${\partial x \over \partial x}=I.$
>       + ${\partial \begin{Vmatrix}x\end{Vmatrix}^2 \over \partial x}=2x.$
>       + ${\partial Ax \over \partial x}=A^T.$
>       + ${\partial x^TA \over \partial x}=A.$
>       + $z_k=f(x_k),\forall k=[1,\cdots,K],x_k\in C,$<br/>${\partial f(x) \over \partial x}={\begin{bmatrix}\partial f(x_j) \over \partial x_i\end{bmatrix}_{K \times K}}={\begin{bmatrix}f'(x_1)&0&\cdots&0\\0&f'(x_2)&\cdots&0\\\vdots&\vdots&\ddots&0\\0&0&\cdots&f(x_K)\end{bmatrix}}=diag(f'(x)).$
>    + Logistic函数:
>    + Softmax函数
> ----
> #### Appendix.C 数学优化(Mathematical Optimization)
> + 数学优化(Mathematical Optimization)问题/最优化问题：指在一定约束条件下，求解一个目标函数的最值问题.
>    + 定义:
>----
> #### Appendix.D 概率论
> + 样本空间：一个随机试验所有可能结果的集合.随机试验中的每个可能结果称为*样本点*.
> + 随机事件/事件：一个被赋予*概率*的事物集合，或者说是样本空间中的一个子集.
> + 概率(Probability)：表示一个*随机事件*发生的可能性大小，为0到1之间的实数.
> + 随机变量：用一个数X表示随机试验的结果，X的取值随试验结果的不同而变化，是样本点的一个函数.
>    + 离散随机变量：随机变量X有N个*有限可列举*的取值<!-- $\lbrace x_1,...,x_N \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clbrace%20x_1%2C...%2Cx_N%20%5Crbrace">.
>       + 离散随机变量的概率分布：<!-- $p(X_n)=P(X=x_n), \sum_{n=1}^N p(x_n)=1, p(x_n) \ge 0, \forall n \in \lbrace 1,...,N \rbrace.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p(X_n)%3DP(X%3Dx_n)%2C%20%5Csum_%7Bn%3D1%7D%5EN%20p(x_n)%3D1%2C%20p(x_n)%20%5Cge%200%2C%20%5Cforall%20n%20%5Cin%20%5Clbrace%201%2C...%2CN%20%5Crbrace."> 常见离散随机变量的概率分布：
>         + 伯努利分布(Bernoulli Distribution)：<!-- $p(x)=\mu^x(1-\mu)^{(1-x)}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p(x)%3D%5Cmu%5Ex(1-%5Cmu)%5E%7B(1-x)%7D."><br/>x取值0或1(事件A不发生或发生)，μ(或1-μ)：一次试验中事件A发生(或不发生)的平均概率.
>         + 二项分布(Binomial Distribution)：<!-- $p(X=k)=\begin{pmatrix}N\\k\end{pmatrix}\mu^k(1-\mu)^{(N-k)},k=0,\cdots,N.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p(X%3Dk)%3D%5Cbegin%7Bpmatrix%7DN%5C%5Ck%5Cend%7Bpmatrix%7D%5Cmu%5Ek(1-%5Cmu)%5E%7B(N-k)%7D%2Ck%3D0%2C%5Ccdots%2CN."><br/>在N次伯努利试验中发生k次事件A的概率.<!-- $\begin{pmatrix}N\\k\end{pmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bpmatrix%7DN%5C%5Ck%5Cend%7Bpmatrix%7D">表示组合总数.<br/>
>         \* 排列组合：<br/>
>         (1) *排列*(Permutation|Arrangement)是指从*给定个数*的元素中取出*指定个数*的元素进行*排序*，N个不同元素可以有N！种不同的*排列方式*，即*N的阶乘*：<!-- $N!\triangleq N \times (N-1) \times \cdots \times 3 \times 2 \times 1.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=N!%5Ctriangleq%20N%20%5Ctimes%20(N-1)%20%5Ctimes%20%5Ccdots%20%5Ctimes%203%20%5Ctimes%202%20%5Ctimes%201."><br/>如果从*N个元素*中取出*k个元素*，这k个元素的排列总数为<!-- $p_N^k \triangleq N \times (N-1) \times \cdots \times (N-k+1) = \frac{N!}{(N-k)!}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p_N%5Ek%20%5Ctriangleq%20N%20%5Ctimes%20(N-1)%20%5Ctimes%20%5Ccdots%20%5Ctimes%20(N-k%2B1)%20%3D%20%5Cfrac%7BN!%7D%7B(N-k)!%7D."><br/>(2)*组合*(Combination)与*排列*的区别在于*组合*不考虑排序，只考虑从N个元素中取出k个元素可能存在的*组合数*，因此除去k个元素可以有的k！个排列方式：<!-- $C_N^k \triangleq {k \choose N}=\frac{p_N^k}{k!}=\frac{N!}{(N-k)!k!}=\frac{N \times (N-1) \times \cdots \times (N-k+1)}{k \times (k-1) \times \cdots \times 1}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=C_N%5Ek%20%5Ctriangleq%20%7Bk%20%5Cchoose%20N%7D%3D%5Cfrac%7Bp_N%5Ek%7D%7Bk!%7D%3D%5Cfrac%7BN!%7D%7B(N-k)!k!%7D%3D%5Cfrac%7BN%20%5Ctimes%20(N-1)%20%5Ctimes%20%5Ccdots%20%5Ctimes%20(N-k%2B1)%7D%7Bk%20%5Ctimes%20(k-1)%20%5Ctimes%20%5Ccdots%20%5Ctimes%201%7D."><br/><!-- $C_N^k = C_N^{N-k}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=C_N%5Ek%20%3D%20C_N%5E%7BN-k%7D"><br/><!-- $C_N^{k-1}+C_N^k=C_{N+1}^k$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=C_N%5E%7Bk-1%7D%2BC_N%5Ek%3DC_%7BN%2B1%7D%5Ek"><br/><!-- $C_N^0+C_N^1+ \cdots +C_N^N=2^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=C_N%5E0%2BC_N%5E1%2B%20%5Ccdots%20%2BC_N%5EN%3D2%5EN"><br/><!-- $C_k^k+C_{k+1}^k+ \cdots +C_N^k=C_{N+1}^{k+1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=C_k%5Ek%2BC_%7Bk%2B1%7D%5Ek%2B%20%5Ccdots%20%2BC_N%5Ek%3DC_%7BN%2B1%7D%5E%7Bk%2B1%7D"><br/><!-- $\sum_{i=0}^k C_n^i C_m^{k-i}=C_{n+m}^k$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Csum_%7Bi%3D0%7D%5Ek%20C_n%5Ei%20C_m%5E%7Bk-i%7D%3DC_%7Bn%2Bm%7D%5Ek">
>    + 连续随机变量：随机变量X的取值*不可列举*，由全部实数或一部分区间构成，<!-- $X=\lbrace x|a \le x \le b \rbrace, -\infty \le a \le b \le \infty.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=X%3D%5Clbrace%20x%7Ca%20%5Cle%20x%20%5Cle%20b%20%5Crbrace%2C%20-%5Cinfty%20%5Cle%20a%20%5Cle%20b%20%5Cle%20%5Cinfty."> 与离散随机变量不同的是，连续随机变量X取一个具体值<!-- $x_i$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x_i">的概率为0.
>       + 概率密度函数(Probability Density Function.PDF)：<br/>连续随机变量的概率分布，p(x)为可积函数(可微 <!-- $\Rightarrow$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow">可积)，<br/><!-- ${\int_{-\infty}^{+\infty}p(x)dx}=1, p(x) \ge 0.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%2B%5Cinfty%7Dp(x)dx%7D%3D1%2C%20p(x)%20%5Cge%200."><br/>给定概率密度函数p(x)，便可以计算出随机变量落入某一区域的概率. <!-- $\mathcal{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BR%7D">表示x的非常小的邻域，<!-- $|\mathcal{R}|$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7C%5Cmathcal%7BR%7D%7C">表示<!-- $\mathcal{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BR%7D">的大小，则<!-- $p(x)|\mathcal{R}|$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p(x)%7C%5Cmathcal%7BR%7D%7C">可以反映随机变量X处于区域<!-- $\mathcal{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BR%7D">的概率大小. 常见连续随机变量概率分布：
>       + 均匀分布(Uniform Distribution)：<br/><!-- $p(x)=\begin{cases}\frac{1}{b-a}&,a\le x \le b\\0 & ,x<a \text{ or } x>b\end{cases}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p(x)%3D%5Cbegin%7Bcases%7D%5Cfrac%7B1%7D%7Bb-a%7D%26%2Ca%5Cle%20x%20%5Cle%20b%5C%5C0%20%26%20%2Cx%3Ca%20%5Ctext%7B%20or%20%7D%20x%3Eb%5Cend%7Bcases%7D">
>       + 正态分布(Normal Distribution|Gaussian Distribution)：<br/><!-- $X \backsim \mathcal{N}(\mu,\sigma^2),p(x)=\frac{1}{\sqrt{2 \pi \sigma}}e^{-\frac{(x-\mu)^2}{2 \sigma^2}},\mu,\sigma \in C,(\sigma > 0).$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=X%20%5Cbacksim%20%5Cmathcal%7BN%7D(%5Cmu%2C%5Csigma%5E2)%2Cp(x)%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma%7D%7De%5E%7B-%5Cfrac%7B(x-%5Cmu)%5E2%7D%7B2%20%5Csigma%5E2%7D%7D%2C%5Cmu%2C%5Csigma%20%5Cin%20C%2C(%5Csigma%20%3E%200)."><br/>Standard Normal Distribution: <!-- $X \backsim \mathcal{N}(0,1)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=X%20%5Cbacksim%20%5Cmathcal%7BN%7D(0%2C1)">
>    + 累计分布函数(Cumulative Distribution Function, CDF): 随机变量X的取值≤x的概率.<br/><!-- $cdf(x)=p(X\le x)=\int_{-\infty}^x p(t)dt.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=cdf(x)%3Dp(X%5Cle%20x)%3D%5Cint_%7B-%5Cinfty%7D%5Ex%20p(t)dt.">
> + 随机向量：一组*随机变量*构成的向量:K维*随机向量*<!-- $X=[X_1,X_2,\cdots,X_K]$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=X%3D%5BX_1%2CX_2%2C%5Ccdots%2CX_K%5D">，一维随机向量即随机变量.
>   + 离散随机向量-联合概率分布(Joint Probability Distribution)：<br/><!-- $P(X_1=x_1,X_2=x_2,\cdots,X_K=x_K)=p(x_1,x_2,\cdots,x_K)\ge 0,\forall x_1 \in \Omega_1,x_2 \in \Omega_2,\cdots,x_K \in \Omega_K.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P(X_1%3Dx_1%2CX_2%3Dx_2%2C%5Ccdots%2CX_K%3Dx_K)%3Dp(x_1%2Cx_2%2C%5Ccdots%2Cx_K)%5Cge%200%2C%5Cforall%20x_1%20%5Cin%20%5COmega_1%2Cx_2%20%5Cin%20%5COmega_2%2C%5Ccdots%2Cx_K%20%5Cin%20%5COmega_K."><br/><!-- $\sum_{x_1 \in \Omega_1,x_2 \in \Omega_2,\cdots,x_K \in \Omega_K}p(x_1,x_2,\cdots,x_K)=\sum_{x_1 \in \Omega_1}\sum_{x_2 \in \Omega_2} \cdots \sum_{x_K \in \Omega_K}p(x_1,x_2,\cdots,x_K)=1.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Csum_%7Bx_1%20%5Cin%20%5COmega_1%2Cx_2%20%5Cin%20%5COmega_2%2C%5Ccdots%2Cx_K%20%5Cin%20%5COmega_K%7Dp(x_1%2Cx_2%2C%5Ccdots%2Cx_K)%3D%5Csum_%7Bx_1%20%5Cin%20%5COmega_1%7D%5Csum_%7Bx_2%20%5Cin%20%5COmega_2%7D%20%5Ccdots%20%5Csum_%7Bx_K%20%5Cin%20%5COmega_K%7Dp(x_1%2Cx_2%2C%5Ccdots%2Cx_K)%3D1.">
>      + 多项分布(Multinomial Distribution)：e.g.,An urn contains 8 red balls, 3 yellow balls and 9 white balls. 6 balls are randomly selected with/without replacement. What is the probability 2 are red, 1 is yellow, and 3 are white?<br/>(With Replacement)<!-- $P(X_1=x_1,X_2=x_2,\cdots,X_K=x_K)=\frac{n!}{x_1!x_2!\cdots x_K!}p_1^{x_1}p_2^{x_2}\cdots p_K^{x_K},n:the number of balls; K:the number of colors.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P(X_1%3Dx_1%2CX_2%3Dx_2%2C%5Ccdots%2CX_K%3Dx_K)%3D%5Cfrac%7Bn!%7D%7Bx_1!x_2!%5Ccdots%20x_K!%7Dp_1%5E%7Bx_1%7Dp_2%5E%7Bx_2%7D%5Ccdots%20p_K%5E%7Bx_K%7D%2Cn%3Athe%20number%20of%20balls%3B%20K%3Athe%20number%20of%20colors."><br/><!-- $p(X_1=2,X_1=1,X_3=3)=\frac{6!}{2!1!3!}(\frac{8}{20})^2(\frac{3}{20})^1(\frac{9}{20})^3$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p(X_1%3D2%2CX_1%3D1%2CX_3%3D3)%3D%5Cfrac%7B6!%7D%7B2!1!3!%7D(%5Cfrac%7B8%7D%7B20%7D)%5E2(%5Cfrac%7B3%7D%7B20%7D)%5E1(%5Cfrac%7B9%7D%7B20%7D)%5E3"><br/>(Without Replacement)<!-- $P(X_1=x_1,X_2=x_2,\cdots,X_K=x_K)=\frac{\left(\frac{2}{8}\right)\left(\frac{1}{3}\right)\left(\frac{3}{9}\right)}{\left(\frac{6}{20}\right)}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P(X_1%3Dx_1%2CX_2%3Dx_2%2C%5Ccdots%2CX_K%3Dx_K)%3D%5Cfrac%7B%5Cleft(%5Cfrac%7B2%7D%7B8%7D%5Cright)%5Cleft(%5Cfrac%7B1%7D%7B3%7D%5Cright)%5Cleft(%5Cfrac%7B3%7D%7B9%7D%5Cright)%7D%7B%5Cleft(%5Cfrac%7B6%7D%7B20%7D%5Cright)%7D.">
>   + 连续随机向量-联合概率密度函数(Joint Probability Density Function)：一个K维连续随机变量X的联合概率分布满足:<br/><!-- $p(x)=p(x_1,x_2,\cdots,x_K) \ge 0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p(x)%3Dp(x_1%2Cx_2%2C%5Ccdots%2Cx_K)%20%5Cge%200"><br/><!-- $\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}p(x_1,\cdots,x_K)d_1 \cdots d_K=1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D%5Ccdots%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7Dp(x_1%2C%5Ccdots%2Cx_K)d_1%20%5Ccdots%20d_K%3D1">
>      + 多元正态/高斯分布(Multivariate Normal|Gaussian Distribution)：<br/>e.g.$X=[x_1,x_2]^T,\mu=[\mu_1,\mu_2]^T,\Sigma^2=\begin{bmatrix}\sigma_{11}&\sigma_{12}\\\sigma_{21}&\sigma_{22}\end{bmatrix}=\begin{bmatrix}\sigma_1^2&\sigma_{12}\\\sigma_{21}&\sigma_2^2\end{bmatrix}$<br/>$\sigma_{12}=0,\rho_{12}=0,|\Sigma|=\sigma_1^2\sigma_2^2$<br/>$F(x_1,x_2)=p(x_1)p(x_2)={\frac{1}{\sqrt{2 \pi \sigma_1}}e^{-\frac{(x_1-\mu)^2}{2 \sigma_1^2}}}{\frac{1}{\sqrt{2 \pi \sigma_2}}e^{-\frac{(x_2-\mu)^2}{2 \sigma_2^2}}}=$
$\frac{1}{(2 \pi)^{\frac 22}(\sigma_1^2 \sigma_2^2)^{\frac 12}}e^{-\frac 12[\frac{(x_1-\mu_1)^2}{\sigma_1^2}+\frac{(x_2-\mu_2)^2}{\sigma_2^2}]}=$
>      + 各项同性高斯分布()：
>      + 狄利克雷分布()：
> + 边际分布：
> + 条件概率分布：
> + 贝叶斯定理：
> + 独立与条件独立：
> + 期望与方差：
> + 随机过程：
>    + 马尔可夫过程：
>    + 高斯过程：
>----
> #### Appendix.E 信息论
> +
>----
> Chap.1 
> Chap.2 机器学习概述<br/>


> Chap.4 前馈神经网络

> Chap.5 卷积神经网络
